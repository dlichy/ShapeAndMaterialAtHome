
<!-- saved from url=(0037)https://guanyingc.github.io/SDPS-Net/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./Self-calibrating Deep Photometric Stereo Networks_files/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1000px;
}	
h1 {
    font-weight:300;
}

.disclaimerbox {
    background-color: #eee;		
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
}

video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
}

a:link,a:visited
{
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
  }

  td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
  }

  .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>



<title>Shape and Material Capture at Home</title>
<meta property="og:image" content="https://github.com/dlichy/ShapeAndMaterial"> <!--TODO-->
<!--<meta property="og:title" content="Self-calibrating Deep Photometric Stereo Networks, In CVPR 2019.">-->
</head>

<body>
<br>
<center>
    <span style="font-size:42px">Shape and Material Capture at Home</span><br>
    <table align="center" width="900px">
        <tbody><tr>
            <td align="center" width="900px">
                <span style="font-size:22px"><a href="http://www.cs.umd.edu/~dlichy/">Daniel Lichy<sup>1</sup></a></span>  
                <span style="font-size:22px"><a href="">Jiaye Wu<sup>1</sup></a></span>  
                <span style="font-size:22px"><a href="https://homes.cs.washington.edu/~soumya91/">Soumyadip Sengupta<sup>2</sup></a></span>  
                <span style="font-size:22px"><a href="http://www.cs.umd.edu/~djacobs/">David W. Jacobs<sup>1</sup></a></span>  
            </td>
        </tr>
    </tbody></table>
    <table align="center" width="800px">
        <tbody><tr><td align="center" width="300px">
            <span style="font-size:21px"><sup>1</sup>Univerity of Maryland <sup>2</sup>University of Washington</span> <br>
        </td>
    </tr></tbody></table>
    <br>
    <table align="center" width="900px">
        <tbody><tr>
            <td align="center" width="900px">
                <center>
                    <span style="font-size:22px"><a href="https://github.com/dlichy/ShapeAndMaterial">Code [PyTorch]</a></span>    
                    <span style="font-size:22px"><a href="https://arxiv.org/abs/2104.06397">Paper [CVPR 2021]</a> </span>
		    <span style="font-size:22px"><a href="https://www.youtube.com/watch?v=80CL4Xbwyuw">Video [CVPR 2021]</a> </span>    
                    <br>
            </td>
        </tr>
    </tbody></table>
</center>
<br>

<table align="center" width="1000px">
    <tbody><tr>
        <td align="center" width="1200px">
            <img class="round" style="height:330px" src="./images/teaser2.png">
        </td>
    </tr> 
</tbody></table>
<hr>

<center><h1>Abstract</h1></center><p>
	In this paper, we present a technique for estimating the geometry and reflectance of objects using only a camera, flashlight, and optionally a tripod. We propose a simple data capture technique in which the user goes around the object, illuminating it with a flashlight and capturing only a few images.Our main technical contribution is the introduction of a recursive neural architecture, which can predict geometry and reflectance at 2<sup>k</sup>x2<sup>k</sup> resolution given an input image at 2<sup>k</sup>x2<sup>k</sup> and estimated geometry and reflectance from the previous step at 2<sup>k-1</sup>x2<sup>k-1</sup>. This recursive architecture, termed RecNet, is trained with 256x256 resolution but can easily operate on 1024x1024 images during inference. 
We show that our method produces more accurate surface normal and albedo, especially in regions of specular highlights and cast shadows, compared to previous approaches, given three or fewer input images.
    </p><table align="center" width="900px">
    
    
</table>
<hr>

	
	

<center><h1>Capture Process</h1></center><table align="center">
    
    <tbody><tr>
        <td align="center" width="1000px">
            <!--<img class="round" style="width:1000px" src="./images/capture_process.mp4">-->
            <video width="860" height="540" autoplay muted loop>
            	<source src="./images/capture_process.mp4">
            </video>
        </td>
    </tr>
    <tr>
        <td>
        <p><br>We introduce a weakly calibrated capture procedure where a user shines a flashlight at an object from (up to) six approximate directions: right, front-right, front (co-located with the camera), front-left, left, and above.</p>
        </td>
    </tr>
</tbody></table>
<table align="center">
    <tbody><tr>
        <td align="center" width="1000px">
            <!--<img class="round" style="width:1000px" src="./images/video_soldier.mp4">-->
            <video width="860" height="540" autoplay muted loop>
            	<source src="./images/video_soldier.mp4">
            </video>
        </td>
    </tr>
    <tr>
        <td>
        <p><br>Another example of the capture process and <b>high resolution</b> results.</p>
        </td>
    </tr>
</tbody></table>
<hr>

<center><h1>Recursive Network Architecture</h1></center><table align="center">
    
    <tbody><tr> <td align="center">
            <video width="860" height="540" controls autoplay muted loop>
            	<source src="./images/net_figure.mp4">
            </video>
    </td> </tr>
    <tr> <td>
    <p><br>Our recursive network architecture learns to predict normals at resolution 2<sup>k</sup>x2<sup>k</sup> given an image at 2<sup>k</sup>x2<sup>k</sup> and normals at  2<sup>k-1</sup>x2<sup>k-1</sup>. This enables us to train on low resolution (256x256) data and generalize to high resolution (1024x1024) data at test time.<p>
    </td> </tr>
</tbody></table>
<hr>

<center><h1>Six Image Results</h1></center>
    
    
    <table align="center" width="1030px">
        <tbody> <!--<tr> <td align="center">
                <img class="round" style="width:900px" src="./Self-calibrating Deep Photometric Stereo Networks_files/diligent_main_quant.png">
        </td> </tr> -->
        <tr><td align="center">
                <br> Co-located input image (first row). Novel View (second row). Novel View diffuse only (third row). <br>
                <img class="round" style="height:200px" src="./images/squirrel_cup/images_s5_b0_ch6.png">
                <img class="round" style="height:200px" src="./images/bear_cup/images_s5_b0_ch6.png">
                <img class="round" style="height:200px" src="./images/black_skull/images_s5_b0_ch6.png">
                <img class="round" style="height:200px" src="./images/white_skull/images_s5_b0_ch6.png">
                <img class="round" style="height:200px" src="./images/hat/images_s5_b0_ch6.png">
        </td></tr>
        
        <tr><td align="center">
               <img class="round" style="height:200px" src="./images/squirrel_cup/object_motion_video_gif.gif">
               <img class="round" style="height:200px" src="./images/bear_cup/object_motion_video_gif.gif">
               <img class="round" style="height:200px" src="./images/black_skull/object_motion_video_gif.gif">
               <img class="round" style="height:200px" src="./images/white_skull/object_motion_video_gif.gif">
               <img class="round" style="height:200px" src="./images/hat/object_motion_video_gif.gif">
        </td></tr>
        
        <tr><td align="center">
               <img class="round" style="height:200px" src="./images/squirrel_cup/object_motion_video_diffuse_gif.gif">
               <img class="round" style="height:200px" src="./images/bear_cup/object_motion_video_diffuse_gif.gif">
               <img class="round" style="height:200px" src="./images/black_skull/object_motion_video_diffuse_gif.gif">
               <img class="round" style="height:200px" src="./images/white_skull/object_motion_video_diffuse_gif.gif">
               <img class="round" style="height:200px" src="./images/hat/object_motion_video_diffuse_gif.gif">
        </td></tr>
        
    </tbody></table>
<hr>

<center><h1>Comparison with State-of-the-Art Uncalibrated Photometric Stereo</h1></center>
            <center><video width="860" height="540" controls autoplay muted loop>
            	<source src="./images/comparison_videos/multi_image_compare.mp4">
            </video></center>

<hr>
<center><h1>Single-Image Results</h1></center>
    <center> Images taken with iPhone and built-in flash </center>
    <table align="center" width="420px">
    
        <tr><td align="center">
                <br> Input image (first row). Novel View (second row). Novel View diffuse only (third row). <br>
                <img class="round" style="height:200px" src="./images/single_skull/images_s5_b0_ch6.png">
                <img class="round" style="height:200px" src="./images/single_starwars_cup/images_s5_b0_ch6.png">
        </td></tr>
        
        <tr><td align="center">
               <img class="round" style="height:200px" src="./images/single_skull/object_motion_video_gif.gif">
               <img class="round" style="height:200px" src="./images/single_starwars_cup/object_motion_video_gif.gif">
        </td></tr>
        
        <tr><td align="center">
               <img class="round" style="height:200px" src="./images/single_skull/object_motion_video_diffuse_gif.gif">
               <img class="round" style="height:200px" src="./images/single_starwars_cup/object_motion_video_diffuse_gif.gif">
        </td></tr>
        
    </tbody></table>
    
<hr>
<center><h1>Comparison with State-of-the-Art Single (or Two) Shot Inverse Rendering Methods
            <video width="860" height="540" controls autoplay muted loop>
            	<source src="./images/comparison_videos/single_image_compare.mp4">
            </video>
</center>


<hr>
<center><h1>Code and Model</h1></center>
    <center><h3>Code, models and datasets are available at <a href="https://github.com/dlichy/ShapeAndMaterial">Github!</a></h3></center>
    <!--<h3>Our code and trained model will be made publicly available before June 2019! !</h3>-->
<hr>

<table align="center" width="1000px">
    <tbody><tr>
        <td width="300px">
           <left>
            <center><h1>Acknowledgments</h1></center>
		<center> This research is supported by the National Science Foundation under grant no. IIS-1526234 and IIS-1910132.</center>
            </left>
        </td>
    </tr>
</tbody></table>
<br>

<p style="text-align:center;font-size:16px;">
    Webpage template borrowed from <a href="https://guanyingc.github.io/SDPS-Net/">Self-calibrating Deep Photometric Stereo Networks</a>.
</p>


</body></html>
